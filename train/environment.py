from pathlib import Path
import math

import numpy as np
from PIL import Image
import gymnasium as gym

import torch
import torch.nn as nn
from torch import Tensor
from torchvision import transforms

from action import Action
from model import MainModel
from position import Position
from client import AutoPilotClient

CKPT_PATH_COMBINE = Path("./pretrain/bn/1741430443-ln=skill-loss=8.477-e3.pth")
STATE_CONFIG = {
    "region_coords": {
        "blood":    (51, 11, 230, 36),
        "Shield":   (51, 45, 230, 70),
        "mana":     (51, 78, 230, 103),
        "mini_map": (1060, 120, 1258, 321),
        "self":     (1144, 204, 1176, 236)
    },
    "thresholds": {
        "blood":    [410000, 380000, 360000],
        "shield":   [570000, 530000, 490000, 460000, 430000, 390000],
        "mana":     [510000, 450000, 390000],
        "self_combat": 37000
    }
}

class FeatureExtractor(nn.Module):
    def __init__(self):
        super(FeatureExtractor, self).__init__()
        self.effi_net = MainModel().effi_net
        checkpoint = torch.load(CKPT_PATH_COMBINE, weights_only=False)
        self.effi_net.load_state_dict(checkpoint.state_dict(), strict=False)
    def forward(self, x):
        return self.effi_net(x)
    
if __name__ == "__main__":
    model = FeatureExtractor().to("cuda")
    model.eval()
    with torch.no_grad():
        res = model(torch.randn(10, 3, 1280, 720).to("cuda"))
        print(res.cpu().numpy().shape)

class SoulKnightEnv(gym.Env):
    def __init__(self, device, autopilot_config: dict):
        super(SoulKnightEnv, self).__init__()
        
        self.device = device
        self.client = AutoPilotClient("SKM_16448", autopilot_config)
        self.feature_extractor = FeatureExtractor().to(self.device)
        self.feature_extractor.eval()
        
        self.observation_space = gym.spaces.Box(
            low=-float("inf"),
            high=float("inf"),
            shape=(10, 1280),
            dtype=np.float32
        )
        
        self.features = torch.zeros((1, 10, 1280), dtype=torch.float32).to("cpu")

        # is_move[0-1], move_angle[0-255], is_attack[0-1], is_skill[0-1]
        self.action_space = gym.spaces.MultiDiscrete([2, 256, 2, 2])
        self.running_timer = 0
        self.in_portal = False
        
        self.steps = 0
        self.rewards = np.zeros((128), dtype=np.float32)
        
    def show_reward(self) -> float:
        self.steps += 1
        if self.steps % 128 == 0:
            print(f"reward: {self.rewards.mean().item():.4f}")
            
    
    def reset(self, *args, **kwargs):
        """ é‡ç½®ç¯å¢ƒå¹¶è¿”å›åˆå§‹è§‚æµ‹ """
        # 1. è·å–åŸå§‹æ¸¸æˆç”»é¢
        task_res = False
        task_res = self.client.try_task("restart", timeout=60, max_retry=1)
        
        features, raw_frame = self.fetch_features()
        # 3. åˆå§‹åŒ–çŠ¶æ€è·Ÿè¸ª
        self.prev_state = self._extract_game_state(raw_frame)
        self.running_duration = 0

        return features, {"suceess": task_res}

    def step(self, action: np.ndarray) -> tuple[np.ndarray, float, bool, bool, dict]:
        """ æ‰§è¡ŒåŠ¨ä½œå¹¶è¿”å›ç¯å¢ƒåé¦ˆ """
        # 1. æ‰§è¡Œæ¸¸æˆåŠ¨ä½œ
        self.client.sync_action(Action.from_raw(action))
        # 2. è·å–æ–°çŠ¶æ€
        features, new_frame = self.fetch_features()
        new_state = self._extract_game_state(new_frame)
        # 3. è®¡ç®—å¥–åŠ±
        reward = self._calculate_reward(new_state)
        # select buff
        if self._check_portal_state(new_state):
            self.client.sync_click(Position(640, 640))
            self.client.sync_click(Position(640, 640))
        # 4. åˆ¤æ–­å›åˆç»ˆæ­¢
        terminated = self._check_done_condition(new_state)
        truncated = False                                                           # TODO: impl truncated
        # 5. æ›´æ–°çŠ¶æ€è·Ÿè¸ª
        self.prev_state = new_state
        self.current_frame = new_frame # ?
        self.running_duration += 1
        # houyao
        self.show_reward()
        self.rewards = np.roll(self.rewards, 1, axis=0)
        self.rewards[-1] = reward
        
        
        
        return features, reward, terminated, truncated, {}
    
    def fetch_features(self) -> tuple[np.ndarray, np.ndarray]:
        """fetch_fb and extract features, then update the feature dequeue
        
        Returns:
            tuple[]:
            (features, raw_frame): np.ndarray (1, 10, 1280), np.ndarray (1, 1280)
        """
        
        new_feature, raw_frame = self.fetch_fb_to_feature() # (1, 1280)
        self.features = torch.roll(self.features, shifts=1, dims=1)
        self.features[:, -1, :] = new_feature.to("cpu")
        return self.features, raw_frame
        
    
    def fetch_fb_to_feature(self) -> tuple[np.ndarray, np.ndarray]:
        new_frame = self.client.fetch_fb()
        processed_frame = self._preprocess_frame(new_frame)
        with torch.no_grad():
            feature = self.feature_extractor(Tensor(processed_frame).unsqueeze(0).to(self.device))
        return feature, new_frame

    def _preprocess_frame(self, raw_frame: np.ndarray) -> np.ndarray:
        """ å›¾åƒé¢„å¤„ç†æµæ°´çº¿ï¼ˆå…³é”®ä¿®æ”¹ç‚¹ï¼‰ """
        data_transform = {
            "train": transforms.Compose([
                transforms.ToTensor(),
                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])}
        processed_frame = data_transform["train"](raw_frame)

        return processed_frame

    def _extract_game_state(self, frame: np.ndarray) -> dict:
        """ ä»åŸå§‹å¸§æå–æ¸¸æˆçŠ¶æ€ï¼ˆç”¨äºå¥–åŠ±è®¡ç®—ï¼‰ """
        pil_image = Image.fromarray(frame)
        state = {}

        # è®¡ç®—å„åŒºåŸŸç°åº¦å€¼
        for name, coords in STATE_CONFIG["region_coords"].items():
            region = pil_image.crop(coords)
            gray_region = region.convert('L')
            state[name] = np.sum(np.array(gray_region))

        # çŠ¶æ€åˆ¤æ–­é€»è¾‘
        state["in_portal"]      = self._check_portal_state(state)
        state["combat_state"]   = state["self"] <= STATE_CONFIG["thresholds"]["self_combat"]
        state["blood_level"]    = self._quantize_value(state["blood"],  STATE_CONFIG["thresholds"]["blood"])
        state["shield_level"]   = self._quantize_value(state["Shield"], STATE_CONFIG["thresholds"]["shield"])
        state["mana_level"]     = self._quantize_value(state["mana"],   STATE_CONFIG["thresholds"]["mana"])

        return state

    def _calculate_reward(self, current_state: dict) -> float:
        """ åŸºäºçŠ¶æ€å˜åŒ–çš„å¥–åŠ±è®¡ç®— """
        reward = 0.0

        # ä¼ é€é—¨å¥–åŠ±ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰
        if current_state["in_portal"] and not self.prev_state["in_portal"]:
            return 5.0

        # è¡€é‡å˜åŒ–
        blood_diff = current_state["blood_level"] - self.prev_state["blood_level"]
        if current_state["blood_level"] == 0:
            reward -= 100
        elif blood_diff < 0:
            if self.prev_state["blood_level"] == 2 and current_state["blood_level"] == 1:
                reward -= 20
        elif blood_diff > 0:
            reward += abs(blood_diff) * 10

        # ç›¾é‡å˜åŒ–
        shield_diff = current_state["shield_level"] - self.prev_state["shield_level"]
        reward += shield_diff * (-1 if shield_diff < 0 else 2)

        # è“é‡å˜åŒ–
        mana_diff = current_state["mana_level"] - self.prev_state["mana_level"]
        if mana_diff < 0:
            if self.prev_state["mana_level"] == 2 and current_state["mana_level"] == 1:
                reward -= 1
            elif self.prev_state["mana_level"] == 1 and current_state["mana_level"] == 0:
                reward -= 20
        elif mana_diff > 0:
            reward += abs(mana_diff) * 5

        # çŠ¶æ€è½¬æ¢å¥–åŠ±
        if not current_state["combat_state"] and self.prev_state["combat_state"]:
            reward += 1
            self.running_duration = 0
            
        if current_state["combat_state"] and not self.prev_state["combat_state"]:
            reward += 0.8
            self.running_duration = 0
            
        elif not current_state["combat_state"]:
            reward -= 0.0001 * math.sqrt(self.running_duration)
            self.running_duration += 1
            # if self.running_duration >= 100:  # å‡è®¾æ¯æ­¥0.125ç§’ï¼Œ1.25ç§’=100æ­¥ğŸ®ğŸ®ğŸ®ğŸ®ğŸ®
            #     reward -= 0.01
            #     self.running_duration = 0

        return reward

    def _quantize_value(self, value: float, thresholds: list) -> int:
        """ å°†è¿ç»­å€¼é‡åŒ–ä¸ºç¦»æ•£ç­‰çº§ """
        for i, th in enumerate(thresholds):
            if value > th:
                return len(thresholds) - i
        return 0

    def _check_portal_state(self, state: dict) -> bool:
        """ åˆ¤æ–­æ˜¯å¦åœ¨ä¼ é€é—¨ä¸­ """
        portal_cond1 = (state["blood"] == state["Shield"] == state["mana"])
        portal_cond2 = (state["blood"] < 3e5 or state["Shield"] < 3e5 or state["mana"] < 3e5)
        return portal_cond1 or portal_cond2

    def _check_done_condition(self, state: dict) -> bool:
        """ åˆ¤æ–­å›åˆæ˜¯å¦ç»“æŸ """
        return state["blood_level"] == 0  # ç©ºè¡€æ—¶ç»“æŸ